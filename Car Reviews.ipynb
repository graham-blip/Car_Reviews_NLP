{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bbb7ecb",
   "metadata": {},
   "source": [
    "# Car Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e187bbf",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This workbook illustrates how a classifier can be built to conduct sentiment anlysis on car reviews. Each review is labelled with either ‘Pos’ or ‘Neg’ to indicate whether the review has been assessed as positive or negative in the sentiment it expresses. The workbook first begins with prepping the data to allow for a general purpose model to classfiy the reveiws. In part one a Multinomial Naive Bayes model is used to classify the reviews. In part two a new model is introduced in an attempt to improve upon the resuts from part one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe4d23df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431b4b76",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "251d5efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('car-reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d93cda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neg</td>\n",
       "      <td>In 1992 we bought a new Taurus and we really ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Neg</td>\n",
       "      <td>The last business trip  I drove to San Franci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Neg</td>\n",
       "      <td>My husband and I purchased a 1990 Ford F250 a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Neg</td>\n",
       "      <td>I feel I have a thorough opinion of this truc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neg</td>\n",
       "      <td>AS a mother of 3  all of whom are still in ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment                                             Review\n",
       "0       Neg   In 1992 we bought a new Taurus and we really ...\n",
       "1       Neg   The last business trip  I drove to San Franci...\n",
       "2       Neg   My husband and I purchased a 1990 Ford F250 a...\n",
       "3       Neg   I feel I have a thorough opinion of this truc...\n",
       "4       Neg   AS a mother of 3  all of whom are still in ca..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbf110da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neg    691\n",
       "Pos    691\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The positive and negative are euqally dispersed\n",
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702ccd35",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7b5729",
   "metadata": {},
   "source": [
    "Start by converting the positive and negative reviews into a binary encoding of 0 and 1 where Negative is 0 and Positive is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b6c2dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    691\n",
       "1    691\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Binary coding\n",
    "df['Sentiment'] = np.where(df['Sentiment']=='Neg', 0, 1)\n",
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a05c9cc",
   "metadata": {},
   "source": [
    "The next step is to split the data into testing and training sets. This is done at the begining of the workbook to prevent any data leakage.  \n",
    "\n",
    "The data is randomly split into 80% training and 20% testing. (Use a seed (55) so that we can reproduce results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f8c3e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.Review, df.Sentiment, test_size=0.2, random_state = 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44893948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dimensions: ((1105,), (1105,))\n",
      "Test dimensions: ((277,), (277,))\n",
      "1    556\n",
      "0    549\n",
      "Name: Sentiment, dtype: int64\n",
      "0    142\n",
      "1    135\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f'Train dimensions: {X_train.shape, y_train.shape}')\n",
    "print(f'Test dimensions: {X_test.shape, y_test.shape}')\n",
    "\n",
    "# Check out target distribution\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b1b917",
   "metadata": {},
   "source": [
    "Rounding up gives us an equal split of 80% in the training set and 20% in the test set. However, it is important to note that now the split between positive and negative reviews is no longer exactly 50:50."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f89e53",
   "metadata": {},
   "source": [
    "## Part One"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b54c84",
   "metadata": {},
   "source": [
    "# 1.\n",
    "\n",
    "### Removal of words and punctuation that do not affect sentiment and all words are lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b8df292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of RegexpTokenizer for alphanumeric tokens\n",
    "tokeniser = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4a74d1",
   "metadata": {},
   "source": [
    "To demonstrate how we will use stemming and prevent altering the origional training set then create a copy of the X_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8a2879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo =  X_train.copy()\n",
    "demo.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9d57f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        im not exactly an unbiased author  ive owned ...\n",
       "1        I am a young woman and a first time car buyer...\n",
       "2        This vehicle was a first time lease for my hu...\n",
       "3        Currently own Dark Wedgewood  Eddie Bauer Edi...\n",
       "4        We bought this vehicle 6 months ago after the...\n",
       "                              ...                        \n",
       "1100     I bought this car new in 1997 to replace my N...\n",
       "1101     I bought this car as a repairable for  3k and...\n",
       "1102     I am a soccer mom  And I got this car at a di...\n",
       "1103     This car is terrible  It is falling apart and...\n",
       "1104      NOTE read this part of the review first then...\n",
       "Name: Review, Length: 1105, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2fb14d",
   "metadata": {},
   "source": [
    "A demonstration of how the each review will be processed and stemmed will be shown on the first reviews. This process will then be scaled up and conducted on all the reviews. However, as the data set is large in the demo stage only the first review will be printed out and examples of lower casing and stemming will be shown on this review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2d3ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to print out the first review\n",
    "# print(demo[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fd4c49",
   "metadata": {},
   "source": [
    "### Sample of how tokenisation, stemming and removal of stopwords is conducted.  \n",
    "\n",
    "A function called sample_stem is created for the purpose of this demo. A more genral purpose function that performs the same steps is later created when processing all the data.  \n",
    "\n",
    "#### Tokenisation\n",
    "\n",
    "In this funciton the first step is to take the entire review which is currently one string and to tokenise this. This takes each word and places it into its own string.  \n",
    "\n",
    "#### Stemming\n",
    "\n",
    "Stemming is essentailly trimming a word down in an attempt to achieve a 'stem' of the word. In this case the porter stemming algorithm is used (Porter, 2006). While it is important to note that not all the stems are real English langugage and to the reader may not make sense. The purpose of this algorithm is to find a common stem between words which are similar. However, in many cases it is possible to demonstate (as shown below in the examples from the first review) that many different variations of the same word are can be condensed into one stem.  \n",
    "\n",
    "#### Lower case\n",
    "\n",
    "Here this is done in the same step as stemming but it is simply taking all words and ensuring they are lower case.  \n",
    "\n",
    "#### Removal of stop words \n",
    "\n",
    "In this step the nltk stopwords corpus is used. It has a total of 179 stop words for English for example: 'we' or 'you' are all considered stopwords that do not contribute much to the sentiment of the review. Subsequently, these stop words are stripped from each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c65111ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_stem(review):\n",
    "    tokens = tokeniser.tokenize(review)\n",
    "    porter = PorterStemmer()\n",
    "    #Lower case and stemming is done in one step\n",
    "    stems = [porter.stem(token.lower()) for token in tokens]\n",
    "    print(\"Number of stems: \")\n",
    "    print(len(stems))\n",
    "    #Using list comprehension we can remove all stop words\n",
    "    keywords = [stem for stem in stems if stem not in stopwords.words('english')]\n",
    "    print('The first 10 keywords are now: ')\n",
    "    print(keywords[:10])\n",
    "    print('The number of keywords is: ')\n",
    "    print(len(keywords))\n",
    "    return tokens, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0146fd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stems: \n",
      "2068\n",
      "The first 10 keywords are now: \n",
      "['im', 'exactli', 'unbias', 'author', 'ive', 'corral', 'late', 'model', 'perform', 'mustang']\n",
      "The number of keywords is: \n",
      "1213\n"
     ]
    }
   ],
   "source": [
    "first_review = sample_stem(demo[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052cc638",
   "metadata": {},
   "source": [
    "# 2.\n",
    "\n",
    "### Words with the same stem are treated as variations of the same stem. \n",
    "\n",
    "#### Demonstration on 3 different stems:\n",
    "\n",
    "- mustang\n",
    "- comfort\n",
    "- servic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cc682a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count each keyword from previous step\n",
    "post_stem = {word: first_review[1].count(word) for word in set(first_review[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f68136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with pre stem \n",
    "pre_stem = {word: first_review[0].count(word) for word in set(first_review[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "167158f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With stemming:\n",
      "\n",
      "mustang = 14\n",
      "mustangs = None\n"
     ]
    }
   ],
   "source": [
    "#Display the key and value\n",
    "print('With stemming:')\n",
    "print(\"\")\n",
    "keys = ['mustang','mustangs']\n",
    "for key in keys:\n",
    "    print(key,'=', post_stem.get(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f21632d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without stemming:\n",
      "\n",
      "Mustang = 8\n",
      "Mustangs = 6\n"
     ]
    }
   ],
   "source": [
    "print('Without stemming:')\n",
    "print(\"\")\n",
    "keys = ['Mustang','Mustangs']\n",
    "for key in keys:\n",
    "    print(key,'=', pre_stem.get(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd57d850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With stemming:\n",
      "\n",
      "comfort = 2\n",
      "comfortable = None\n",
      "Comfort = None\n",
      "Comfortable = None\n"
     ]
    }
   ],
   "source": [
    "#Display the key and value\n",
    "print('With stemming:')\n",
    "print(\"\")\n",
    "keys = ['comfort','comfortable', 'Comfort', 'Comfortable']\n",
    "for key in keys:\n",
    "    print(key,'=', post_stem.get(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19034541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without stemming:\n",
      "\n",
      "comfort = None\n",
      "comfortable = 1\n",
      "Comfort = 1\n",
      "Comfortable = None\n"
     ]
    }
   ],
   "source": [
    "print('Without stemming:')\n",
    "print(\"\")\n",
    "keys = ['comfort','comfortable', 'Comfort', 'Comfortable']\n",
    "for key in keys:\n",
    "    print(key,'=', pre_stem.get(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ff6496c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With stemming:\n",
      "\n",
      "servicing = None\n",
      "service = None\n",
      "servic = 4\n",
      "services = None\n"
     ]
    }
   ],
   "source": [
    "print('With stemming:')\n",
    "print(\"\")\n",
    "keys = ['servicing','service','servic','services']\n",
    "for key in keys:\n",
    "    print(key,'=', post_stem.get(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4f05481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without stemming:\n",
      "\n",
      "servicing = 1\n",
      "service = 2\n",
      "servic = None\n",
      "services = 1\n"
     ]
    }
   ],
   "source": [
    "print('Without stemming:')\n",
    "print(\"\")\n",
    "keys = ['servicing','service','servic','services']\n",
    "for key in keys:\n",
    "    print(key,'=', pre_stem.get(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba0d875",
   "metadata": {},
   "source": [
    "Now that we have demonstrated how stemming works we create a function which takes the text as an input and can transfrom the full training set tokenising, removing punctuation, convert to lowercase, stemming and removal of stop words.  \n",
    "\n",
    "Placing it into a function will help with the pipeline when then trying to repeat the process with the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28a96747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    '''\n",
    "    This function takes some text as an input and returns a tokenised set stripped from punctuation,\n",
    "    capital letters and stop words.\n",
    "    '''\n",
    "    # Tokenise words while ignoring punctuation\n",
    "    tokeniser = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokeniser.tokenize(text)\n",
    "    \n",
    "    # Porter stemming\n",
    "    porter = PorterStemmer()\n",
    "    stems = [porter.stem(token.lower()) for token in tokens]\n",
    "    \n",
    "    \n",
    "    # Remove stop words\n",
    "    keywords = [stem for stem in stems if stem not in stopwords.words('english')]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d48c136",
   "metadata": {},
   "source": [
    "# 3.\n",
    "\n",
    "### Output to illustrate a vector for each review has been created.  \n",
    "\n",
    "Each element in the vector is either a binary variable indicating the presence of a word/stem or the number of times it appears. Only a small sample of reviews will be displayed.  \n",
    "\n",
    "Prior to placing the elements into a vector it is helpful to transform each word into a score of some form that will allow for a better method of defining how important each word is in context of the review. In this workbook term frequency-inverse document frequency (Tf-IDf) is used.  \n",
    "\n",
    "The Tf-IDf score which is given to each word in every review is based on how many times each word apppears in each review but then reduced when it occurrs in other reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953422e3",
   "metadata": {},
   "source": [
    "### Term frequency-inverse document frequency formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a842a",
   "metadata": {},
   "source": [
    " The formula for the term frequency is as follows:  \n",
    "\n",
    "$$tf(w,d) = \\log(1+f(w,d))$$\n",
    "\n",
    "where $d$ is the document or in this case each review, $w$ is the word and the function f is the frequency.  \n",
    "\n",
    "The inverse term frequency is then calculated:  \n",
    "\n",
    "$$idf(w, D) = \\log(\\frac{N}{f(w,D)})$$\n",
    "\n",
    "where N is the number of reviews, D is the collection of all reviews.  \n",
    "\n",
    "Lastly, the score is:  \n",
    "\n",
    "\n",
    "$$tfidf(w,d,D) = tf(w,d) \\times idf(w,D)$$  \n",
    "\n",
    "This is now applied to the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ed57d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1105, 10023)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of TfidfVectorizer\n",
    "vectoriser = TfidfVectorizer(analyzer=preprocess_text)\n",
    "# Fit to the data and transform to feature matrix\n",
    "X_train_tfidf = vectoriser.fit_transform(X_train)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2480203",
   "metadata": {},
   "source": [
    " A sample of the first thirty scores from the vector can be seen below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc4a5125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Tf-IDf\n",
      "6l         0.221023\n",
      "amp        0.201446\n",
      "mustang    0.198180\n",
      "gt         0.153084\n",
      "perform    0.146717\n",
      "hatchback  0.124528\n",
      "exclud     0.120055\n",
      "model      0.117719\n",
      "0l         0.114280\n",
      "4          0.111049\n",
      "91         0.108844\n",
      "2000       0.103618\n",
      "system     0.103168\n",
      "packag     0.094137\n",
      "25k        0.090041\n",
      "rear       0.089710\n",
      "gaug       0.088516\n",
      "rev        0.088416\n",
      "option     0.087536\n",
      "structur   0.085307\n",
      "curiou     0.085307\n",
      "style      0.084744\n",
      "rigid      0.080288\n",
      "get        0.080132\n",
      "im         0.079742\n",
      "provid     0.079349\n",
      "mayb       0.079120\n",
      "wheel      0.078813\n",
      "due        0.076333\n",
      "rpm        0.075411\n"
     ]
    }
   ],
   "source": [
    "df_tdif = pd.DataFrame(X_train_tfidf[0].T.todense(), index=vectoriser.get_feature_names(), columns=[\"Tf-IDf\"])\n",
    "df_tdif = df_tdif.sort_values('Tf-IDf', ascending=False)\n",
    "print (df_tdif.head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5267c6",
   "metadata": {},
   "source": [
    "# 4. \n",
    "\n",
    "### Use the Multinomial Naive Bayes Model to classify the car reivews\n",
    "\n",
    "This is a probabilistic learning method that is used to apply textual analysis. It is based on Bayes theorem and is used to predict the senitiment of the car review positive or negative.  \n",
    "\n",
    "The settings of this model is used with the [defaults](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html). In this case $\\alpha = 1.0$ for Laplace smoothing to ensure that the model can deal with words that do not appear in the test set or training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8b2e8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_NB = MultinomialNB()\n",
    "mnb_clf = M_NB.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "883b8a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.760181   0.79638009 0.78280543 0.74660633 0.79638009]\n",
      "Accuracy: 0.78 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "mnb_clf_scores = cross_val_score(mnb_clf, X_train_tfidf, y_train, cv=5)\n",
    "print(mnb_clf_scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (mnb_clf_scores.mean(), mnb_clf_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b038dd29",
   "metadata": {},
   "source": [
    "Initial model seems to fit with an accuracy of 78% (+/- 4% of the time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49f45b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_clf_pred = cross_val_predict(mnb_clf, X_train_tfidf, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb4d057",
   "metadata": {},
   "source": [
    "# 5.\n",
    "\n",
    "### Labled confusion matrix showing the performance of Naive Bayes classifier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1f1c733",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_NBtraining, fp_NBtraining, fn_NBtraining, tp_NBtraining = confusion_matrix(y_train, mnb_clf_pred).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "839884de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of True positives: 467\n",
      "Number of True negatives: 391\n",
      "Number of False positives: 158\n",
      "Number of False negatives: 89\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of True positives:\", tp_NBtraining)\n",
    "print(\"Number of True negatives:\", tn_NBtraining)\n",
    "print(\"Number of False positives:\", fp_NBtraining)\n",
    "print(\"Number of False negatives:\", fn_NBtraining)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d987d7",
   "metadata": {},
   "source": [
    "Use the test data to see how the model performs on an unseen test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "641ab218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectoriser',\n",
       "                 TfidfVectorizer(analyzer=<function preprocess_text at 0x7fa988e9e1f0>)),\n",
       "                ('classifier', MultinomialNB())])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([('vectoriser', vectoriser),\n",
    "                 ('classifier', mnb_clf)])\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a79c795e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.81\n"
     ]
    }
   ],
   "source": [
    "y_test_pred_NB = pipe.predict(X_test)\n",
    "print(\"Accuracy: %0.2f\" % (accuracy_score(y_test, y_test_pred_NB)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168172af",
   "metadata": {},
   "source": [
    "Here we can see that the Naive Bayes model performs with an accuracy of 81% while it is actually higher than the average of the training percentage it is still nonetheless within the two standard deviations of the training model. Overall, this is a good output especially since the test results closely matches the training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "798ef86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_NBtest, fp_NBtest, fn_NBtest, tp_NBtest = confusion_matrix(y_test, y_test_pred_NB).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6ba6ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of True positives: 111\n",
      "Number of True negatives: 112\n",
      "Number of False positives: 30\n",
      "Number of False negatives: 24\n"
     ]
    }
   ],
   "source": [
    "#The confusion matrix\n",
    "print(\"Number of True positives:\", tp_NBtest)\n",
    "print(\"Number of True negatives:\", tn_NBtest)\n",
    "print(\"Number of False positives:\", fp_NBtest)\n",
    "print(\"Number of False negatives:\", fn_NBtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756fe118",
   "metadata": {},
   "source": [
    "## Part Two"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac44e121",
   "metadata": {},
   "source": [
    "# 1.\n",
    "\n",
    "### Using stochastic gradient descent to find a better performing model to classify the car reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0a9714",
   "metadata": {},
   "source": [
    "In part two stochastic gradient descent will be used in an attempt to fit multiple different models with a range of different hyper paramters. The goal is to select the best performing model on the training data. This will then be compared to the origional multinomial naive bayes model from part one.\n",
    "\n",
    "[Stochastic gradient descent](https://scikit-learn.org/stable/modules/sgd.html) works by taking a random training point for each iteration and fits the gradient to the data. It will update this gradient for each iteration. It will attempt to approximate the true gradient $E(w,b)$ for the data. For each iteration the model parameters are updated as follows:\n",
    "\n",
    "$$w \\leftarrow w - \\eta \\left[\\alpha \\frac{\\partial R(w)}{\\partial w}\n",
    "+ \\frac{\\partial L(w^T x_i + b, y_i)}{\\partial w}\\right]$$\n",
    "\n",
    "$\\eta:$ is the learning rate, $b:$ is the intercept, $R:$ the penalty parameter, $L:$ the loss function and $\\alpha:$ a non negative regulisation hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eb9e9b",
   "metadata": {},
   "source": [
    "# 2.\n",
    "\n",
    "### Stochastic gradient descent with its default settings:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478a1e92",
   "metadata": {},
   "source": [
    "In the default case, the stochastic gradient descent loss function used is a linear Support Vector Machine (SVM). The data points that are closer to the hyperplane are called support vectors which then influence the position and orientation of the hyperplane (Vandewalle, 1999). The default penalty is $l2$ which shrinks but does not eliminate any coefficients. The constant $\\alpha$ that is used to multiply the regularization term is 0.0001. The intercept is set at True and will be fitted. Early stopping is False meaning the learning algorithm will not stop learning even if the model is getting worse at prediciting the correct results.  \n",
    "\n",
    "Here there is no need to scale the data as textual data has its own scale by default (number of times a word appears)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd81ddaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79279279 0.76576577 0.77477477 0.86486486 0.81081081 0.82727273\n",
      " 0.79090909 0.78181818 0.78181818 0.79090909]\n",
      "Accuracy: 0.80 (+/- 0.06)\n"
     ]
    }
   ],
   "source": [
    "# fit the baseline model\n",
    "sgd_clf = SGDClassifier(random_state=55)\n",
    "sgf_clf_scores = cross_val_score(sgd_clf, X_train_tfidf, y_train, cv=10)\n",
    "print(sgf_clf_scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (sgf_clf_scores.mean(), sgf_clf_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b959a3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[429 120]\n",
      " [108 448]]\n"
     ]
    }
   ],
   "source": [
    "sgf_clf_pred = cross_val_predict(sgd_clf, X_train_tfidf, y_train, cv=5)\n",
    "print(confusion_matrix(y_train, sgf_clf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8dfcca",
   "metadata": {},
   "source": [
    "# 3.\n",
    "\n",
    "### Using grid search cross validation to improve the results of the SGD Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0af3580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'early_stopping': True, 'fit_intercept': True, 'loss': 'log', 'penalty': 'l2'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = {'fit_intercept': [True,False],\n",
    "        'early_stopping': [True, False],\n",
    "        'loss' : ['hinge', 'log', 'squared_hinge'],\n",
    "        'penalty' : ['l2', 'l1', 'none']}\n",
    "search = GridSearchCV(estimator=sgd_clf, param_grid=grid, cv=5)\n",
    "search.fit(X_train_tfidf, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f5e66f",
   "metadata": {},
   "source": [
    "After testing a range of different parameters the best loss function is now the logistic regression model, with early stopping (fit intercept and pentaly $l2$ remain unchanged)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dbfed6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77375566 0.80995475 0.83257919 0.81447964 0.7918552 ]\n",
      "Accuracy: 0.80 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "grid_sgd_clf_scores = cross_val_score(search.best_estimator_, X_train_tfidf, y_train, cv=5)\n",
    "print(grid_sgd_clf_scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (grid_sgd_clf_scores.mean(), grid_sgd_clf_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f489510f",
   "metadata": {},
   "source": [
    "The accuracy of this new model on the training data is 80% (as before with the linear SVM. However, there is a slight improvement in the margin of error now only +/- 4% rather than +/- 6% earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7add213",
   "metadata": {},
   "source": [
    "# 4.\n",
    "\n",
    "### Looking at the test data results with the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0e1bde8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectoriser',\n",
       "                 TfidfVectorizer(analyzer=<function preprocess_text at 0x7fa988e9e1f0>)),\n",
       "                ('classifier',\n",
       "                 SGDClassifier(early_stopping=True, loss='log',\n",
       "                               random_state=55))])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2 = Pipeline([('vectoriser', vectoriser),\n",
    "                 ('classifier', search.best_estimator_)])\n",
    "pipe2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "afd21eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78\n"
     ]
    }
   ],
   "source": [
    "y_test_pred_SGD = pipe2.predict(X_test)\n",
    "print(\"Accuracy: %0.2f\" % (accuracy_score(y_test, y_test_pred_SGD)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "39354365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of True positives: 107\n",
      "Number of True negatives: 108\n",
      "Number of False positives: 34\n",
      "Number of False negatives: 28\n"
     ]
    }
   ],
   "source": [
    "tn_SGDtest, fp_SGDtest, fn_SGDtest, tp_SGDtest = confusion_matrix(y_test, y_test_pred_SGD).ravel()\n",
    "print(\"Number of True positives:\", tp_SGDtest)\n",
    "print(\"Number of True negatives:\", tn_SGDtest)\n",
    "print(\"Number of False positives:\", fp_SGDtest)\n",
    "print(\"Number of False negatives:\", fn_SGDtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2d8915",
   "metadata": {},
   "source": [
    "The test data results show an accuracy of 78%, 107 correct positives (cases where there is positve review) and 108 correct negatives (negative reviews). With 34 false positives and 28 false negatives.\n",
    "\n",
    "Interestringly, while using a SDG Classifier it did perform better than Naive Bayes on the training data, it nonetheless has an accuracy rate of 3% lower in the test data. It also has 4 more false positives 34 instead of 30 in part one's Multinomial NB model. The model in part two also has 4 more false negatives 28 rather than the 24 in part one.  \n",
    "\n",
    "Although we expected better performance from fitting a range of different models with a support vector machine or logistic regression using stochastic gradient descent and then further improving these models by applying cross validation to find the best parameters. These better results where only seen in the training data and did not hold up for the test data.  \n",
    "\n",
    "These results suggest that in part two we are in fact overfitting the training data and although we recieved better results this did not transalate into a better model overall and subseqently led to more false positives and false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b273b9a1",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2035fdd9",
   "metadata": {},
   "source": [
    "In this project we have shown how a supervised classification model can be used to classify car reivews. The initial data set contains 691 reviews labeled positive and 691 reviews labeled negative. This was split into a training set 80% and a test set 20%. The data was then preprocessed to better enable a classification model to be fitted on the training set. Two models where used the first a Multinomial Niave Bayes and the second a Stochastic Gradient Descent model.\n",
    "\n",
    "While the second model perfroms better on the trianing data in practice we still get a better result for the MultinomialNB model in the testing data. This suggests that the new model is perhaps overfitting the training data and therefore does not hold up when using the test data. Although it was expected that the second model would perform better on the training data the \"naive\" element of the Naive Bayes model is perhaps what really prevents this model from overfitting the training data and ultimately providing a better result on the test set. Overall, it must be pointed out that the accuracy of both are within the error margins from both the first and second models training data where we output errors margines of within 4%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e5b438",
   "metadata": {},
   "source": [
    "## References  \n",
    "\n",
    "Porter, M.F., 2006. An algorithm for suffix stripping. Program.  \n",
    "\n",
    "Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.  \n",
    "\n",
    "Suykens, J.A. and Vandewalle, J., 1999. Least squares support vector machine classifiers. Neural processing letters, 9(3), pp.293-300."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
